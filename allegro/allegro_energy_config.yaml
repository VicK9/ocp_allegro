### This config file specifies allegro model params. Num of layers, dimensions, etc... ###
### We want to experiment with this ###

### Dataset-relevant values
num_types: 84
avg_num_neighbors: 37

# -- network --
model_builders:
 - allegro.model.Allegro
 # the typical model builders from `nequip` can still be used:
 #  - PerSpeciesRescale
 # - RescaleEnergyEtc

# cutoffs
r_max: 6.0
# avg_num_neighbors: auto

# radial basis
BesselBasis_trainable: true
PolynomialCutoff_p: 6  # sets it BOTH for the RadialBasisProjection AND the Allegro_Module

# symmetry
l_max: 2
parity: o3_full  # allowed: o3_full, o3_restricted, so3

# Allegro layers:
num_layers: 3
env_embed_multiplicity: 32
embed_initial_edge: true

two_body_latent_mlp_latent_dimensions: [64, 128, 256, 512]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_mlp_initialization: uniform

latent_mlp_latent_dimensions: [512]
latent_mlp_nonlinearity: silu
latent_mlp_initialization: uniform
latent_resnet: true

env_embed_mlp_latent_dimensions: []
env_embed_mlp_nonlinearity: null
env_embed_mlp_initialization: uniform

# - end allegro layers -

# Final MLP to go from Allegro latent space to edge energies:
edge_eng_mlp_latent_dimensions: [128]
edge_eng_mlp_nonlinearity: null
edge_eng_mlp_initialization: uniform


### This is all commented as specifying the training procedure goes through the OC20 config file ###
### This may still inform what hyperparameters we may want to pick in the training .yaml file ###
### For an example look at configs/s2ef/2M/allegro.yaml ### 


# -- data --
# dataset: npz                                                                       # type of data set, can be npz or ase
# dataset_url: http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip             # url to download the npz. optional
# dataset_file_name: ./benchmark_data/aspirin_ccsd-train.npz                         # path to data set file
# key_mapping:
#   z: atomic_numbers                                                                # atomic species, integers
#   E: total_energy                                                                  # total potential eneriges to train to
#   F: forces                                                                        # atomic forces to train to
#   R: pos                                                                           # raw atomic positions
# npz_fixed_field_keys:                                                              # fields that are repeated across different examples
#   - atomic_numbers

# A mapping of chemical species to type indexes is necessary if the dataset is provided with atomic numbers instead of type indexes.

# logging
# wandb: false
# wandb_project: aspirin
# verbose: debug

# training
# n_train: 950
# n_val: 50
# batch_size: 5
# max_epochs: 1000000
# learning_rate: 0.002
# train_val_split: random
# shuffle: true
# metrics_key: validation_loss

# use an exponential moving average of the weights
# use_ema: true
# ema_decay: 0.99
# ema_use_num_updates: true

# loss function
# loss_coeffs:
#   forces: 1.
#   total_energy:
#     - 1.
#     - PerAtomMSELoss

# optimizer
# optimizer_name: Adam
# optimizer_params:
#   amsgrad: false
#   betas: !!python/tuple
#   - 0.9
#   - 0.999
#   eps: 1.0e-08
#   weight_decay: 0.

# # lr scheduler, drop lr if no improvement for 50 epochs


# # early stopping if max 7 days is reached or lr drops below 1e-5 or no improvement on val loss for 100 epochs
# early_stopping_upper_bounds:
#   cumulative_wall: 604800.

# early_stopping_lower_bounds:
#   LR: 1.0e-5

# early_stopping_patiences:
#   validation_loss: 100

