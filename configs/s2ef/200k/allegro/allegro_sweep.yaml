
### This config file specifies the training procedure paremeters. ###
### This is the other place where we can fiddle with values/optimize hyperparams. ###


includes:
- configs/s2ef/200k/base.yml

model:
  name: allegro
  cutoff: 6.0
  use_pbc: True
  config_path: 'allegro/allegro_forces_config.yaml'

# *** Important note ***
#   The total number of gpus used for this run was 8.
#   If the global batch size (num_gpus * batch_size) is modified
#   the lr_milestones and warmup_steps need to be adjusted accordingly.

# *** We did not do the multiplication ^ *** #
# Something to consider #

# For finding optim params, it is worth to look into the configs for the other models for a given dataset size (2M in this case)

optim:
  batch_size: 32
  eval_batch_size: 32
  num_workers: 16
  lr_initial: 0.002
  ema_decay: 0.99

  max_epochs: 1
  force_coefficient: 1
  
  #Scheduler
  lr_scheduler_name: ReduceLROnPlateau
  mode: min
  factor: 0.8
  patience: 3
  
  optimizer: Adam
  optimizer_params:
    amsgrad: false
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.
