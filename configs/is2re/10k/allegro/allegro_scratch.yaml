
### This config file specifies the training procedure paremeters. ###
### This is the other place where we can fiddle with values/optimize hyperparams. ###


includes:
- configs/is2re/10k/base_scratch.yml

model:
  name: allegro
  cutoff: 6.0
  use_pbc: True
  config_path: 'allegro/allegro_energy_config.yaml'
  regress_forces: False

# *** Important note ***
#   The total number of gpus used for this run was 8.
#   If the global batch size (num_gpus * batch_size) is modified
#   the lr_milestones and warmup_steps need to be adjusted accordingly.

# *** We did not do the multiplication ^ *** #
# Something to consider #

# For finding optim params, it is worth to look into the configs for the other models for a given dataset size (2M in this case)

optim:
  batch_size: 16 # Original batchsize was 24, so 8 GPUs * 4. I can fit around 6 on my GPU, Lisa may do more
  eval_batch_size: 16
  eval_every: 6250
  num_workers: 3
  lr_initial: 0.001
  ema_decay: 0.99


  max_epochs: 7
  #Scheduler
  #lr_scheduler_name: ReduceLROnPlateau
  #lr_scheduler_patience: 50
  #lr_scheduler_factor: 0.5
  scheduler: ReduceLROnPlateau
  mode: min
  factor: 0.8
  patience: 100
  #warmup_steps: 103
  # lr_milestones: # steps at which lr_initial <- lr_initial * lr_gamma
  # - 101
  # - 102
  # - 103


  optimizer: Adam
  optimizer_params:
    amsgrad: false
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.
