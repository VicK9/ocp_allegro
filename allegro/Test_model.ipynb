{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83278757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allegro.model as al\n",
    "from nequip.model import model_from_config\n",
    "from nequip.data import AtomicData\n",
    "import yaml\n",
    "import torch\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69653cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications needed in the nequip install\n",
    "# 1) Install pytorch geometric\n",
    "\n",
    "# 2) nequip/model/_scaling.py\n",
    "# --> comment out lines 257-259\n",
    "\n",
    "# 3) nequip/data/AtomicData.py\n",
    "# --> add at the beginning:\n",
    "# from torch_geometric.data.data import Data as geom_Data\n",
    "# --> add the following after ~ line 571:\n",
    "# elif isinstance(data, geom_Data):\n",
    "#     keys = data.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac735ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6c85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your config path to an allegro config file\n",
    "config_path = '/home/tin/Documents/GitHub/Allegro/allegro/configs/minimal.yaml'\n",
    "with open(config_path, \"r\") as stream:\n",
    "    try:\n",
    "        config  = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e621309e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder: <class 'nequip.data.transforms.TypeMapper'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn.embedding._one_hot.OneHotAtomEncoding'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn.embedding._edge.RadialBasisEdgeEncoding'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._norm_basis.NormalizedBasis'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn.radial_basis.BesselBasis'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn.cutoffs.PolynomialCutoff'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn.embedding._edge.SphericalHarmonicEdgeAttrs'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._allegro.Allegro_Module'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._fc.ScalarMLPFunction'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._fc.ScalarMLPFunction'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._fc.ScalarMLPFunction'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._fc.ScalarMLP'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'allegro.nn._edgewise.EdgewiseEnergySum'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn._atomwise.AtomwiseReduce'>\n",
      "builder type: <class 'type'>\n",
      "builder: <class 'nequip.nn._atomwise.PerSpeciesScaleShift'>\n",
      "builder type: <class 'type'>\n"
     ]
    }
   ],
   "source": [
    "# Needed to intialize a model from config, needs config, initialize, and an optional dataset\n",
    "# The dataset is needed to calculate the average number of neigbors\n",
    "# For this, put something like avg_num_neighbors: 5 in the config to avoid this issue\n",
    "initialize = False\n",
    "model = model_from_config(config, initialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335aac32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'at_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tannin/Desktop/ocp/allegro/Test_model.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tannin/Desktop/ocp/allegro/Test_model.ipynb#ch0000005?line=0'>1</a>\u001b[0m at_data\u001b[39m.\u001b[39mneighbors\n",
      "\u001b[0;31mNameError\u001b[0m: name 'at_data' is not defined"
     ]
    }
   ],
   "source": [
    "at_data.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28e4eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.func.radial_basis.basis.basis.bessel_weights\n",
      "model.func.allegro.latents.0._forward._weight_0\n",
      "model.func.allegro.latents.0._forward._weight_1\n",
      "model.func.allegro.env_embed_mlps.0._forward._weight_0\n",
      "model.func.allegro.linears.0.w\n",
      "model.func.allegro.final_latent._forward._weight_0\n",
      "model.func.edge_eng._module._forward._weight_0\n",
      "model.func.edge_eng._module._forward._weight_1\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e074b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import QM9, take a data sample to test whether we can do a forward pass\n",
    "dataset = QM9('./')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75576d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'atom_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m'''for layer in self.rescale_layers:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    # This means that self.model is RescaleOutputs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    # this will normalize the targets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    if k not in self._remove_from_model_input\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m}'''\u001b[39;00m\n\u001b[1;32m     20\u001b[0m input_data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;66;03m# This is added because there is a commented out portion of the code\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nequip/nn/_rescale.py:140\u001b[0m, in \u001b[0;36mRescaleOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: AtomicDataDict\u001b[38;5;241m.\u001b[39mType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mType:\n\u001b[0;32m--> 140\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nequip/nn/_grad_output.py:82\u001b[0m, in \u001b[0;36mGradientOutput.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     80\u001b[0m     wrt_tensors\u001b[38;5;241m.\u001b[39mappend(data[k])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# run func\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Get grads\u001b[39;00m\n\u001b[1;32m     84\u001b[0m grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# This makes sense for scalar batch-level or batch-wise outputs, specifically because d(sum(batches))/d wrt = sum(d batch / d wrt) = d my_batch / d wrt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,  \u001b[38;5;66;03m# needed to allow gradients of this output during training\u001b[39;00m\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nequip/nn/_graph_mixin.py:354\u001b[0m, in \u001b[0;36mSequentialGraphNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AtomicDataDict\u001b[38;5;241m.\u001b[39mType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtomicDataDict\u001b[38;5;241m.\u001b[39mType:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 354\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nequip/nn/embedding/_one_hot.py:37\u001b[0m, in \u001b[0;36mOneHotAtomEncoding.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: AtomicDataDict\u001b[38;5;241m.\u001b[39mType):\n\u001b[0;32m---> 37\u001b[0m     type_numbers \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAtomicDataDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mATOM_TYPE_KEY\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(\n\u001b[1;32m     39\u001b[0m         type_numbers, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_types\n\u001b[1;32m     40\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mtype_numbers\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdata[AtomicDataDict\u001b[38;5;241m.\u001b[39mPOSITIONS_KEY]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     41\u001b[0m     data[AtomicDataDict\u001b[38;5;241m.\u001b[39mNODE_ATTRS_KEY] \u001b[38;5;241m=\u001b[39m one_hot\n",
      "\u001b[0;31mKeyError\u001b[0m: 'atom_types'"
     ]
    }
   ],
   "source": [
    "# Imitating the data forward pass\n",
    "data = AtomicData.to_AtomicDataDict(data)\n",
    "\n",
    "data_unscaled = data\n",
    "# This block is also an issue, so I have commented it out\n",
    "'''for layer in self.rescale_layers:\n",
    "    # This means that self.model is RescaleOutputs\n",
    "    # this will normalize the targets\n",
    "    # in validation (eval mode), it does nothing\n",
    "    # in train mode, if normalizes the targets\n",
    "    data_unscaled = layer.unscale(data_unscaled)\n",
    "\n",
    "# Run model\n",
    "# We make a shallow copy of the input dict in case the model modifies it\n",
    "input_data = {\n",
    "    k: v\n",
    "    for k, v in data_unscaled.items()\n",
    "    if k not in self._remove_from_model_input\n",
    "}'''\n",
    "input_data = data # This is added because there is a commented out portion of the code\n",
    "out = model(input_data)\n",
    "\n",
    "# Good luck have fun babes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87d0c183b4ce8d1c51046009f4c5da7cb8c0426d243e154946c7b32c0ede7140"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ocp-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
